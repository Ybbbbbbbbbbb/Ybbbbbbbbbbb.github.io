<!DOCTYPE html><html lang="zh-hans"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no"><meta name="author" content="Burn Yang"><meta name="renderer" content="webkit"><meta name="copyright" content="Burn Yang"><meta name="keywords" content="最强影魔升级之路"><meta name="description" content="null"><meta name="Cache-Control" content="no-cache"><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><title>论文笔记:Conﬁdence-Based Data Association and Discriminative Deep Appearance Learning for Robust Online Multi-Object Tracking · Burn's Blog</title><link rel="stylesheet" href="/css/style.css?v=2018.7.9"><link rel="stylesheet" href="/css/animation.css?v=2018.7.9"><link rel="icon" href="/img/assets/dota2.ico"><link rel="stylesheet" href="https://cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.6"><!-- scripts--><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->(function( w ){
  "use strict";
  // rel=preload support test
  if( !w.loadCSS ){
    w.loadCSS = function(){};
  }
  // define on the loadCSS obj
  var rp = loadCSS.relpreload = {};
  // rel=preload feature support test
  // runs once and returns a function for compat purposes
  rp.support = (function(){
    var ret;
    try {
      ret = w.document.createElement( "link" ).relList.supports( "preload" );
    } catch (e) {
      ret = false;
    }
    return function(){
      return ret;
    };
  })();

  // if preload isn't supported, get an asynchronous load by using a non-matching media attribute
  // then change that media back to its intended value on load
  rp.bindMediaToggle = function( link ){
    // remember existing media attr for ultimate state, or default to 'all'
    var finalMedia = link.media || "all";

    function enableStylesheet(){
      link.media = finalMedia;
    }

    // bind load handlers to enable media
    if( link.addEventListener ){
      link.addEventListener( "load", enableStylesheet );
    } else if( link.attachEvent ){
      link.attachEvent( "onload", enableStylesheet );
    }

    // Set rel and non-applicable media type to start an async request
    // note: timeout allows this to happen async to let rendering continue in IE
    setTimeout(function(){
      link.rel = "stylesheet";
      link.media = "only x";
    });
    // also enable media after 3 seconds,
    // which will catch very old browsers (android 2.x, old firefox) that don't support onload on link
    setTimeout( enableStylesheet, 3000 );
  };

  // loop through link elements in DOM
  rp.poly = function(){
    // double check this to prevent external calls from running
    if( rp.support() ){
      return;
    }
    var links = w.document.getElementsByTagName( "link" );
    for( var i = 0; i < links.length; i++ ){
      var link = links[ i ];
      // qualify links to those with rel=preload and as=style attrs
      if( link.rel === "preload" && link.getAttribute( "as" ) === "style" && !link.getAttribute( "data-loadcss" ) ){
        // prevent rerunning on link
        link.setAttribute( "data-loadcss", true );
        // bind listeners to toggle media back
        rp.bindMediaToggle( link );
      }
    }
  };

  // if unsupported, run the polyfill
  if( !rp.support() ){
    // run once at least
    rp.poly();

    // rerun poly on an interval until onload
    var run = w.setInterval( rp.poly, 500 );
    if( w.addEventListener ){
      w.addEventListener( "load", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    } else if( w.attachEvent ){
      w.attachEvent( "onload", function(){
        rp.poly();
        w.clearInterval( run );
      } );
    }
  }


  // commonjs
  if( typeof exports !== "undefined" ){
    exports.loadCSS = loadCSS;
  }
  else {
    w.loadCSS = loadCSS;
  }
}( typeof global !== "undefined" ? global : this ) );</script><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js" defer></script><script src="/js/main.js?v=2018.7.9" defer></script><!-- fancybox--><link rel="preload" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'"><script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js" defer></script><!-- busuanzi--><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><section class="profile-close" id="cxo-profile"><div class="profile-avatar"><i class="fa fa-caret-left"></i><img src="/img/assets/dog.png"></div><!-- hexo-inject:begin --><!-- hexo-inject:end --><!--.profile-saying
  i.fa.fa-comment
  .saying--><div class="cxo-profile-inner"><div class="profile-name">最强影魔</div><div class="profile-signature">for me</div><div class="friends"><div>FRIENDS</div><span><a href="https://github.com/flyingxg" target="_black">son</a></span><span><a href="https://github.com/Ybbbbbbbbbbb" target="_black">me</a></span></div><div class="read-progress"></div></div></section><header id="cxo-intro" style="height: 70vh;background-image: url(/img/assets/sf.jpg);"><nav id="cxo-intro-nav"><section><div class="intro-nav-title"><a href="/">Burn's Blog</a></div><div class="intro-nav-label-box"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div><i class="fa fa-bars intro-nav-menu"><div class="intro-nav-drop"><a href="/">Home</a><a href="/about/">About</a><a href="/archives/">Archives</a><a href="/tags/">Tags</a></div></i><div class="clear"></div></section></nav><h1 class="post-title">论文笔记:Conﬁdence-Based Data Association and Discriminative Deep Appearance Learning for Robust Online Multi-Object Tracking</h1><div class="post-intros"><div class="post-intro-meta"><span class="post-intro-time"><i class="post-intro-calendar fa fa-calendar"></i><span>2019-04-16</span></span><span class="post-intro-tags"><a class="intro-tag fa fa-tag" href="javascript:void(0)" date-tags="-dl -mot"> -dl -mot</a></span></div></div></header><article class="cxo-up" id="cxo-content-outer"><section id="cxo-content-inner"><article class="article-entry" id="post"><h2 id="论文主要结构"><a href="#论文主要结构" class="headerlink" title="论文主要结构"></a>论文主要结构</h2><p>这篇论文是介绍多目标跟踪的online方法。论文主要分为两个部分，第一部分是介绍整个多目标跟踪的框架，第二部分介绍了目标关联的计算方法。接下来按照这两个部分介绍。</p>
<h3 id="跟踪框架"><a href="#跟踪框架" class="headerlink" title="跟踪框架"></a>跟踪框架</h3><ul>
<li><p>1.轨迹置信度。这篇文章提出了一种计算轨迹可靠程度的方法，称为Tracklet Conﬁdence。决定轨迹置信度高低的有三方面因素，分别是轨迹长度，干扰程度，以及文章第三部分的相似度计算。通过公式和他的解释，理解起来很容易。形成的轨迹越长，这条轨迹的可靠程度就越高。如果轨迹被干扰的部分很多，这条轨迹的可靠程度就会下降。还有通过第三部分相似度的计算，相似度高的轨迹置信度越高。轨迹置信度的计算公式如下：<br>$$<br>conf(T_i)\models(\frac{1}{L}\displaystyle\sum_{k\in[t_s^{i}, t_e^{i}],v^{i}(k)=1} \land(T^{i}, z_k^{i})) \times (1 - exp^{-\beta\cdot\sqrt{(L-w)}})<br>$$<br>（公式编辑起来是真的麻烦）</p>
</li>
<li><p>2.High Confidence。在作者前一篇论文中，又把这种方法叫做local association。按照我读论文的理解，无论是local association或是本文中的high confidence。通俗来讲，就是把前后两帧中，将最有可能是一条轨迹的目标与前面已经形成的轨迹关联起来。按照实际视频的跟踪结果来看，这种方法占据了视频帧中绝大部分的目标关联，第三种全局关联所起到的效果就很小了。（不过我还是把公式编辑一下吧，就当练手了吧）<br>$$<br>S = [s_{ij}]<em>{h \times n},s</em>{ij} =-\log(\land(T^{i(hi)}, Z_t^{j}))<br>$$<br>其中Z的含义为在第t帧中的j目标。</p>
</li>
<li><p>3.Low confidence(global association)。这部分内容用来处理根据轨迹置信度方式计算出的低置信度的轨迹，在上一步没有与目标相连的轨迹。这部分轨迹很可能是由于发生遮挡之后，长度变短，轨迹置信度变低。按照论文来看，这条轨迹就只有三个下场，第一是跟高置信度轨迹想连（对此我表示很疑惑，他的意思是说将两个轨迹相连？可是要是能相连的说，说明高置信度轨迹已经缺失了一段了，那还能保持高置信度吗？而且在测试时，发现这部分内容用到的地方不多，虽然加入之后，效果有所提升，但是还是感觉不能理解）第二是这条轨迹终止，第三是把轨迹跟剩余的没有连接的目标相连，（强行配对一哈）。</p>
</li>
</ul>
<h3 id="相似度计算"><a href="#相似度计算" class="headerlink" title="相似度计算"></a>相似度计算</h3><p>​         相似度计算分为三部分，分别是外观模型计算，运动模型计算，目标位置方式计算。</p>
<ul>
<li><p>1.appearance model。 本文采用的是深度学习的方式提取外观特征从而构建外观模型。（在作者上一篇论文中，外观模型采用的是计算bound boxing的颜色直方图，我在项目中采用了颜色直方图+梯度方式的特征去丰富外观模型，效果有所提示，不过数据量还是太小）。在整个外观模型中，作者采用的网络结构为</p>
<p>input layer -&gt; conv (9×9，channel=12) -&gt; pooling(s=2) -&gt; conv(5×5， channel=16) -&gt; pooling(s=2) -&gt; conv(5×5，channel=24) -&gt; pooling(s=2) -&gt; fc(1152, 150) (论文中的图片尺寸计算目前还有问题，具体的网络结构说的也不是很清楚，还需要做实验测试)</p>
<p>损失函数设计：计算两张输入网络后输出图是l2范数。然后再加上最后全连接层的两个参数所构成的惩罚项。<br>$$<br>E = E_1 + E_2 = \frac{1}{2}\sum_{p,q}\Psi(y_{pq}(D_g^2(x_p,x_q)- \tau)) + \frac{\lambda}{2}\sum_{l=1}^{L}(|W^2|_F^2 + |b^l|_2^2), l =1,….,L<br>$$</p>
<p>$$<br>\psi(e) = \frac{1}{\beta} \log(1 + \exp(\beta e))<br>$$</p>
<p>如果两张输入图是同一目标，则y(pq) = 1，otherwise y(pq) = -1。在这篇论文中，作者提出现在其他行人数据集上训练，然后保留前6层的参数，然后在mot的数据集上，训练最后一层的参数。因为刚进入深度学习方向，我对作者的网络细节还不清楚，不明白图像的尺寸变化，以及所训练的参数含义，据我了解，卷积层应该是没有参数需要网络训练的啊，还需要将论文复现后再加深理解。</p>
</li>
<li><p>2.motion model。 运动模型就是采用卡尔曼滤波线性的估计行人的运动情况，仅仅使用了前向运动。也可以采用非线性的运动情况。<br>$$<br>\land^M(X,Y) = N(P_x^{tail} +V_x^F\Theta;P_Y^{head, O^F}) \times N(P_Y^{head} + V_Y^B\Theta;P_X^{tail},O^B)<br>$$</p>
</li>
<li><p>3.size model。 就是计算两个目标间bound boxing的重叠程度。<br>$$<br>\land^S(X, Y) = \exp(-{\frac{h_X -h_Y}{h_X + h_Y} + \frac{w_X - w_Y}{w_X + w_Y}})<br>$$</p>
</li>
</ul>
<p>这就是论文的大致内容。最重要的外观模型神经网络结构我的理解还有问题，等我使用pytorch实现之后，看看能不能使用别的提取特征的网络结构，然后再修改。（第一次写博文，这篇论文有点难懂，写了一天，感觉思路还是有点混乱，继续加油吧）</p>
</article><!-- lincense--><div class="license-wrapper"><p> <span>Author:  </span><a href="https://ybbbbbbbbbbb.github.io">Burn Yang</a></p><p> <span>Link:  </span><a href="https://ybbbbbbbbbbb.github.io/2019/04/16/Conﬁdence-Based-Data-Association-and-Discriminative-Deep-Appearance-Learning-for-Robust-Online-Multi-Object-Tracking/">https://ybbbbbbbbbbb.github.io/2019/04/16/Conﬁdence-Based-Data-Association-and-Discriminative-Deep-Appearance-Learning-for-Robust-Online-Multi-Object-Tracking/</a></p><p> <span>Copyright:  </span><span>All articles in this blog are licensed under <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/3.0">CC BY-NC-SA 3.0</a> unless stating additionally.</span></p></div><div class="post-paginator"><div class="clear"></div></div><div id="comment"></div></section></article><footer id="cxo-footer-outer"><div id="cxo-footer-inner"><p class="footer-container"><span>Site by </span><a href="http://hexo.io"><span>Hexo</span></a><span> | theme </span><a href="https://github.com/Longlongyu/hexo-theme-Cxo"><span>Cxo</span></a></p><i class="fa fa-user"> </i><span id="busuanzi_value_site_uv"></span><span> | </span><i class="fa fa-eye"> </i><span id="busuanzi_value_site_pv"></span></div></footer><!-- catelog--><div class="toc-wrapper" style="top: 70vh;"><div class="toc-catalog"><i class="fa fa-list"> </i><span>CATALOG</span></div><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#论文主要结构"><span class="toc-number">1.</span> <span class="toc-text">论文主要结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#跟踪框架"><span class="toc-number">1.1.</span> <span class="toc-text">跟踪框架</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#相似度计算"><span class="toc-number">1.2.</span> <span class="toc-text">相似度计算</span></a></li></ol></li></ol></div><!-- top--><i class="fa fa-arrow-up close" id="go-up" aria-hidden="true"></i><!-- hexo-inject:begin --><!-- hexo-inject:end --></body></html>